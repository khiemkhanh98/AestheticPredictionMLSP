{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep Cloak adv CIFAR10.ipynb","provenance":[{"file_id":"1tbf8RbaCp1Q5TA1l578f6m5P513VJLed","timestamp":1646607068124},{"file_id":"1w_q7Hg3pQg4I1tWrZEHc_v6JMWv5uo0-","timestamp":1633515355933},{"file_id":"1KqsvcEwX_c2fNO1ds6o7MFo6hhlC0z9X","timestamp":1633078608658}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4876bde639b14da4935ea4d5eda8dc4e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b39a2b8dc8a4edaa6b58d9124a3a900","IPY_MODEL_a0464bf57d39433e8e3e955400e53363","IPY_MODEL_05a7ef7da0b946aabe385bed14b75538"],"layout":"IPY_MODEL_2119c5776f6845d68cc68d88e348868d"}},"0b39a2b8dc8a4edaa6b58d9124a3a900":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c556d004b4534485a8dc2cf8117ba715","placeholder":"​","style":"IPY_MODEL_14588a7f71484249a482d07683c41edc","value":""}},"a0464bf57d39433e8e3e955400e53363":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5d5e35b5fb14f228a60ffca0ce018d2","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_837184181a23477ca2881ba009ea24ec","value":170498071}},"05a7ef7da0b946aabe385bed14b75538":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc806a7c6ba14deaa9ea9d9d0989e3fb","placeholder":"​","style":"IPY_MODEL_de096721838c40f68ec3d9d34ca2e55a","value":" 170499072/? [00:03&lt;00:00, 54149951.65it/s]"}},"2119c5776f6845d68cc68d88e348868d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c556d004b4534485a8dc2cf8117ba715":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14588a7f71484249a482d07683c41edc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5d5e35b5fb14f228a60ffca0ce018d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"837184181a23477ca2881ba009ea24ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc806a7c6ba14deaa9ea9d9d0989e3fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de096721838c40f68ec3d9d34ca2e55a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"id":"WdOjh0EKINim","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647267864671,"user_tz":-60,"elapsed":93063,"user":{"displayName":"Khiem Nguyen Trong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11317374952968502408"}},"outputId":"2659e780-0123-4dd0-bf15-6a7375c22635"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zc2IWkZyNqW1","executionInfo":{"status":"ok","timestamp":1647267888787,"user_tz":-60,"elapsed":24144,"user":{"displayName":"Khiem Nguyen Trong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11317374952968502408"}},"outputId":"ca0b2538-07b2-4fd7-84f1-b8322eb1c898"},"source":["import time\n","import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import scipy\n","\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","import torch\n","torch.manual_seed(2)\n","np.random.seed(2)\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, SubsetRandomSampler, SequentialSampler\n","import torchvision.models as models\n","!pip install torchinfo\n","!pip install torchattacks\n","!pip install pip install grad-cam\n","from torchinfo import summary\n","from torchvision.models.resnet import _resnet,BasicBlock\n","import torchattacks\n","import torchvision.utils\n","import torch.nn.functional as F\n","from pytorch_grad_cam import GradCAM"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.6.3\n","Collecting torchattacks\n","  Downloading torchattacks-3.2.4-py3-none-any.whl (102 kB)\n","\u001b[K     |████████████████████████████████| 102 kB 6.8 MB/s \n","\u001b[?25hInstalling collected packages: torchattacks\n","Successfully installed torchattacks-3.2.4\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting install\n","  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n","Collecting grad-cam\n","  Downloading grad-cam-1.3.7.tar.gz (4.5 MB)\n","\u001b[K     |████████████████████████████████| 4.5 MB 7.1 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from grad-cam) (4.1.2.30)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from grad-cam) (4.63.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from grad-cam) (7.1.2)\n","Collecting ttach\n","  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from grad-cam) (1.21.5)\n","Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from grad-cam) (0.11.1+cu111)\n","Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from grad-cam) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.1->grad-cam) (3.10.0.2)\n","Building wheels for collected packages: grad-cam\n","  Building wheel for grad-cam (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for grad-cam: filename=grad_cam-1.3.7-py3-none-any.whl size=25953 sha256=00cd3cea05df4cf5660be20fa0bd643176499621232cdaca660f50eb6a71699d\n","  Stored in directory: /root/.cache/pip/wheels/30/ab/9c/53c523785edffdc6c61755cf82e0dac3342d0d36190c187894\n","Successfully built grad-cam\n","Installing collected packages: ttach, install, grad-cam\n","Successfully installed grad-cam-1.3.7 install-1.3.5 ttach-0.0.3\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122,"referenced_widgets":["4876bde639b14da4935ea4d5eda8dc4e","0b39a2b8dc8a4edaa6b58d9124a3a900","a0464bf57d39433e8e3e955400e53363","05a7ef7da0b946aabe385bed14b75538","2119c5776f6845d68cc68d88e348868d","c556d004b4534485a8dc2cf8117ba715","14588a7f71484249a482d07683c41edc","b5d5e35b5fb14f228a60ffca0ce018d2","837184181a23477ca2881ba009ea24ec","bc806a7c6ba14deaa9ea9d9d0989e3fb","de096721838c40f68ec3d9d34ca2e55a"]},"id":"NKMGeB11qZW1","executionInfo":{"status":"ok","timestamp":1647267896424,"user_tz":-60,"elapsed":7713,"user":{"displayName":"Khiem Nguyen Trong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11317374952968502408"}},"outputId":"d6229794-e8e6-4fb0-a128-d055ffb512f3"},"source":["transform = transforms.Compose([\n","    # transforms.Resize((224)),\n","    transforms.ToTensor(),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform)\n","train_loader = torch.utils.data.DataLoader(\n","    trainset, batch_size=128, shuffle=True, num_workers=2,pin_memory=True)\n","\n","valset = torchvision.datasets.CIFAR10(\n","    root='./data', train=False, download=True, transform=transform)\n","val_loader = torch.utils.data.DataLoader(\n","    valset, batch_size=64, shuffle=False, num_workers=2, sampler = SequentialSampler(valset.data[5000:10000]),pin_memory=True)\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root='./data', train=False, download=True, transform=transform)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    testset, batch_size=64, shuffle=False, num_workers=2, sampler = SequentialSampler(testset.data[5000:10000]),pin_memory=True)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck')\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4876bde639b14da4935ea4d5eda8dc4e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","metadata":{"id":"DUjKynrDJhi3","executionInfo":{"status":"ok","timestamp":1647267897436,"user_tz":-60,"elapsed":1029,"user":{"displayName":"Khiem Nguyen Trong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11317374952968502408"}}},"source":["from torch.nn import Conv2d,AvgPool2d,Linear,Sequential,Dropout,BatchNorm2d,ModuleList,BatchNorm1d\n","import torch.nn.functional as F\n","import numpy as np\n","import math\n","from torch.autograd import Variable\n","\n","class BasicConv(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n","        super(BasicConv, self).__init__()\n","        self.out_channels = out_planes\n","        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n","        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n","        self.relu = nn.ReLU() if relu else None\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.bn is not None:\n","            x = self.bn(x)\n","        if self.relu is not None:\n","            x = self.relu(x)\n","        return x\n","\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","\n","class ChannelGate(nn.Module):\n","    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n","        super(ChannelGate, self).__init__()\n","        self.gate_channels = gate_channels\n","        self.mlp = nn.Sequential(\n","            Flatten(),\n","            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n","            nn.ReLU(),\n","            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n","            )\n","        self.pool_types = pool_types\n","    def forward(self, x):\n","        channel_att_sum = None\n","        for pool_type in self.pool_types:\n","            if pool_type=='avg':\n","                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n","                channel_att_raw = self.mlp( avg_pool )\n","            elif pool_type=='max':\n","                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n","                channel_att_raw = self.mlp( max_pool )\n","\n","            if channel_att_sum is None:\n","                channel_att_sum = channel_att_raw\n","            else:\n","                channel_att_sum = channel_att_sum + channel_att_raw\n","\n","        scale = torch.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n","        return x * scale\n","\n","def logsumexp_2d(tensor):\n","    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n","    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n","    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n","    return outputs\n","\n","class ChannelPool(nn.Module):\n","    def forward(self, x):\n","        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n","\n","class SpatialGate(nn.Module):\n","    def __init__(self):\n","        super(SpatialGate, self).__init__()\n","        kernel_size = 7\n","        self.compress = ChannelPool()\n","        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n","    def forward(self, x):\n","        x_compress = self.compress(x)\n","        x_out = self.spatial(x_compress)\n","        scale = torch.sigmoid(x_out) # broadcasting\n","        return x * scale\n","\n","class CBAM(nn.Module):\n","    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n","        super(CBAM, self).__init__()\n","        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n","        self.no_spatial=no_spatial\n","        if not no_spatial:\n","            self.SpatialGate = SpatialGate()\n","    def forward(self, x):\n","        x_out = self.ChannelGate(x)\n","        if not self.no_spatial:\n","            x_out = self.SpatialGate(x_out)\n","        return x_out\n","\n","class Base(nn.Module):\n","    def freeze(self):\n","        for param in self.base_model.parameters():\n","                param.requires_grad = False\n","    \n","    def unfreeze(self):\n","        for param in self.base_model.parameters():\n","                param.requires_grad = True\n","    \n","    def attach_fea_out(self,classname,input,output):\n","        self.features.append(output)\n","\n","    def attach_fea_in(self,classname,input,output):\n","        self.features.append(input[0])\n","\n","    def __init__(self,trainable = True,attention=False,base=18):\n","        super(Base,self).__init__()\n","        self.features = []\n","        self.channel_size = []\n","        print(base)\n","        if base == 9:\n","            self.base_model = _resnet('resnet', BasicBlock, [1, 1, 1, 1], False, True)\n","        elif base==18:\n","            self.base_model = models.resnet18(pretrained=False)\n","        else:\n","            self.base_model = models.resnet34(pretrained=False)\n","\n","        used_blocks = ['layer1', 'layer2','layer3','layer4']\n","        unused_blocks = ['avgpool','fc']\n","\n","        for block in used_blocks:\n","            getattr(self.base_model,block).register_forward_hook(self.attach_fea_out)\n","\n","        for block in unused_blocks:\n","             setattr(self.base_model,block,nn.Identity())\n","        \n","        if not trainable:\n","            self.freeze()\n","\n","        fake_img = torch.rand(1,3,256,256) ## pass fake img to the model to get the channel size of each inception block\n","        self.base_model(fake_img)\n","        self.channel_size = [block.size()[1] for block in self.features]\n","        self.features = []\n","\n","    def forward(self,img):\n","        self.base_model(img)\n","\n","    def get_MLSP(self,img,feature_type,resize = True):\n","        self.base_model(img)\n","        if resize:\n","            print(resize)\n","            if feature_type == 'narrow':\n","                MLSP = [F.adaptive_avg_pool2d(block, (1, 1)) for block in self.features]\n","                for i in range(len(MLSP)):\n","                    MLSP[i] = MLSP[i].squeeze(2).squeeze(2)\n","\n","            if feature_type == 'wide':\n","                MLSP = [F.interpolate(block,mode = 'bilinear', size = 7) for block in self.features]\n","            \n","            MLSP = torch.cat(MLSP,dim = 1)\n","            self.features = []\n","        else:\n","            MLSP = self.features\n","            self.features = []\n","        return MLSP\n","\n","\n","\n","class Head(nn.Module):\n","    def conv_block(self,inc,outc,ker,padding = 1,avgpool = False):\n","        modules = []\n","        modules.append(nn.Dropout(0.5))\n","        if avgpool:\n","            modules.append(AvgPool2d(3,1,1))\n","        modules.append(Conv2d(inc,outc,ker,padding = padding))\n","        modules.append(nn.BatchNorm2d(outc))\n","        modules.append(nn.ReLU())\n","        return Sequential(*modules)\n","\n","    def __init__(self,head_type,num_channel):\n","        super(Head, self).__init__()\n","        self.head_type = head_type\n","        self.num_ch = num_channel\n","        if head_type == 'mlsp_cnn_gap_attn':\n","            self.attn = []\n","            self.conv = []\n","            for i in range(4):\n","                if i!=3:\n","                    self.attn.append(CBAM(num_channel[i],reduction_ratio=16))\n","                else:\n","                    self.attn.append(CBAM(num_channel[i],reduction_ratio=16,no_spatial=True))\n","                self.conv.append(Sequential(\n","                                    self.conv_block(num_channel[i],num_channel[i],1,0),\n","                                    self.conv_block(num_channel[i],num_channel[i],3,1),\n","                          ))\n","            self.attn = ModuleList(self.attn)\n","            self.conv = ModuleList(self.conv)\n","        self.dense = Sequential(Linear(960,10))\n","\n","    def forward(self,features):\n","        if self.head_type == 'mlsp_gap':\n","            x = torch.cat([F.adaptive_avg_pool2d(feature, (1, 1)) for feature in features],dim=1)\n","        else:\n","            x = torch.cat([F.adaptive_avg_pool2d(block2(block1(feature)+feature),(1,1)) for feature,block1,block2 in zip(features,self.attn,self.conv)],dim=1)\n","        x = torch.flatten(x, 1)\n","        x = self.dense(x)\n","        return x\n","\n","class Fmodel(nn.Module):\n","    def __init__(self, head_type='mlsp_gap',base = 18):\n","        super(Fmodel,self).__init__()\n","        self.bmodel = Base(base=base)\n","        self.head = Head(head_type,self.bmodel.channel_size)\n","        self.feature_type = 'narrow'    \n","        self.resize = False\n","        self.fea = []\n","        self.gap_fea = []\n","        self.gradient = []\n","        self.handles = []\n","        \n","    def forward(self,img):\n","        x = self.bmodel.get_MLSP(img,self.feature_type,self.resize)\n","        x = self.head(x)\n","        return x\n","\n","    def unfreeze(self):\n","        self.bmodel.unfreeze()\n","    \n","    def freeze(self):\n","        self.bmodel.freeze()\n","\n","    def hook_gap(self):\n","        handle = self.head.dense.register_forward_hook(lambda layer, inl, _,: self.gap_fea.append(inl[0]))\n","        self.handles += [handle]\n","        return handle\n","\n","    def hook_grad(self):\n","        handle = []\n","        handle.append(self.head.conv[0].register_full_backward_hook(lambda layer, inl, out,: self.gradient.append(out[0])))\n","        handle.append(self.head.conv[1].register_full_backward_hook(lambda layer, inl, out,: self.gradient.append(out[0])))\n","        handle.append(self.head.conv[2].register_full_backward_hook(lambda layer, inl, out,: self.gradient.append(out[0])))\n","        handle.append(self.head.conv[3].register_full_backward_hook(lambda layer, inl, out,: self.gradient.append(out[0])))\n","        self.handles += handle\n","        return handle\n","\n","    def hook_fea(self):\n","        handle = []\n","        handle.append(self.head.conv[0].register_forward_hook(lambda layer, inl, out,: self.fea.append(out)))\n","        handle.append(self.head.conv[1].register_forward_hook(lambda layer, inl, out,: self.fea.append(out)))\n","        handle.append(self.head.conv[2].register_forward_hook(lambda layer, inl, out,: self.fea.append(out)))\n","        handle.append(self.head.conv[3].register_forward_hook(lambda layer, inl, out,: self.fea.append(out)))\n","        self.handles += handle\n","        return handle\n","\n","    def hook(self):\n","        self.hook_gap()\n","        self.hook_grad()\n","        self.hook_fea()\n","\n","    def unhook(self):\n","        for fea in self.gap_fea:\n","            fea.detach()\n","        for grad in self.gradient:\n","            grad.detach()\n","        for fea in self.fea:\n","            fea.detach()\n","\n","        self.gap_fea = []\n","        self.gradient = []\n","        self.fea = []\n","\n","        for h in self.handles:\n","            h.remove()\n","        self.handles = []\n","\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ARiAQGKAgJHX"},"source":["This will be the 3 model configs that you should train and generate the attacks on.  "]},{"cell_type":"code","metadata":{"id":"S-mddqsDj8FA","executionInfo":{"status":"ok","timestamp":1647267897439,"user_tz":-60,"elapsed":20,"user":{"displayName":"Khiem Nguyen Trong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11317374952968502408"}}},"source":["import torchattacks\n","\n","def deepcloak(model,mask_sz):\n","    global fea\n","    mask = torch.zeros((mask_sz))\n","    # mask.require_grad = False\n","\n","    atk = torchattacks.PGD(model, eps=8/255, alpha=2/225, steps=7, random_start=True)\n","    #atk = torchattacks.FGSM(model, eps=8/255)\n","    \n","    for inputs,labels in train_loader:\n","        inputs_adv = atk(inputs, labels)\n","        inputs_adv = transforms.functional.normalize(inputs_adv,(0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","        inputs = transforms.functional.normalize(inputs,(0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        \n","        fea = []\n","        model(inputs_adv)\n","        feas_adv = fea[0]\n","        model(inputs)\n","        feas = fea[1]\n","        mask += torch.abs(feas_adv-feas).sum(0).detach().cpu()\n","        for f in fea:\n","            f.detach()\n","    return mask\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxqdi5d1iM3w"},"source":["model = models.resnet18(pretrained=False)\n","model.fc = Linear(512,10)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","class mask_fc(nn.Module):\n","    def __init__(self,fc,mask = torch.ones((512))):\n","        super(mask_fc,self).__init__()\n","        self.mask = mask.to(device)\n","        self.fc = copy.deepcopy(fc)\n","        \n","    def forward(self,x):\n","        return self.fc(self.mask*x)\n","\n","\n","model.load_state_dict(torch.load('/content/gdrive/MyDrive/cifar10/new_saved_model/resnet18_base_0')) \n","model1 = copy.deepcopy(model)\n","model1.to(device)\n","model.to(device)\n","fea = []\n","# model.head.dense.register_forward_hook(lambda layer, inl, _,: fea.append(inl[0].detach()))\n","# mask_sz = 960\n","model.fc.register_forward_hook(lambda layer, inl, _,: fea.append(inl[0].detach()))\n","mask_sz = 512\n","mask = deepcloak(model,mask_sz)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfqzdXPdGCWO"},"source":["percent = 0.05\n","null_ind = torch.topk(mask,int(percent*mask_sz))[1]\n","new_mask = torch.ones((mask_sz))\n","new_mask[null_ind] = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-L7AhexBReJ"},"source":["model.fc = mask_fc(model1.fc,mask = new_mask.to(device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtq8CaryfuM0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646610494630,"user_tz":-60,"elapsed":27852,"user":{"displayName":"Khiem Nguyen Trong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11317374952968502408"}},"outputId":"2b62b04c-896c-402b-e90a-ee2a7ff4bbbc"},"source":["correct = 0 \n","size = 0\n","model.eval().cuda()\n","\n","with  torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        inputs = transforms.functional.normalize(inputs,(0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","        # Make predictions.\n","        prediction= model(inputs)\n","        # prediction= model(inputs,new_mask.to(device))\n","\n","        # Retrieve predictions indexes.\n","        _, predicted_class = torch.max(prediction.data, 1)\n","\n","        # Compute number of correct predictions.\n","        correct += (predicted_class == labels).float().sum().item()\n","        size+=len(prediction)\n","test_accuracy = correct / size\n","print('Test accuracy: {}'.format(test_accuracy))\n","\n","\n","model.eval()\n","atks = [\n","    torchattacks.FGSM(model1, eps=8/255),\n","    torchattacks.PGD(model1, eps=8/255, alpha=2/225, steps=7, random_start=True),\n","]\n","for i in [0,1]:\n","    correct = 0\n","    start = time.time()\n","    size = 0\n","    for images, labels in test_loader:   \n","        adv_images = atks[i](images, labels)\n","        adv_images = transforms.functional.normalize(adv_images,(0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","        labels = labels.to(device)\n","        outputs = model(adv_images)\n","        # outputs = model(adv_images,new_mask.to(device))\n","        _, pre = torch.max(outputs.data, 1)\n","        correct += (pre == labels).float().sum().item()\n","        size+=len(labels)\n","\n","    # print('Total elapsed time (sec): %.2f' % (time.time() - start))\n","    print('Robust accuracy: %.2f ' % (correct / size))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy: 0.7452\n","Robust accuracy: 0.38 \n","Robust accuracy: 0.36 \n"]}]},{"cell_type":"code","source":["for i in range(4):\n","    model = models.resnet18(pretrained=False)\n","    model.fc = Linear(512,10)\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    class mask_fc(nn.Module):\n","        def __init__(self,fc,mask = torch.ones((512))):\n","            super(mask_fc,self).__init__()\n","            self.mask = mask.to(device)\n","            self.fc = copy.deepcopy(fc)\n","            \n","        def forward(self,x):\n","            return self.fc(self.mask*x)\n","\n","\n","    model.load_state_dict(torch.load(f'/content/gdrive/MyDrive/cifar10/new_saved_model/resnet18_base_{i}')) \n","    model1 = copy.deepcopy(model)\n","    model1.to(device)\n","    model.to(device)\n","    fea = []\n","    # model.head.dense.register_forward_hook(lambda layer, inl, _,: fea.append(inl[0].detach()))\n","    # mask_sz = 960\n","    model.fc.register_forward_hook(lambda layer, inl, _,: fea.append(inl[0].detach()))\n","    mask_sz = 512\n","    mask = deepcloak(model,mask_sz)\n","\n","    percent = 0.05\n","    null_ind = torch.topk(mask,int(percent*mask_sz))[1]\n","    new_mask = torch.ones((mask_sz))\n","    new_mask[null_ind] = 0\n","    model.fc = mask_fc(model1.fc,mask = new_mask.to(device))\n","\n","    correct = 0 \n","    size = 0\n","    model.eval().cuda()\n","\n","    with  torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            inputs = transforms.functional.normalize(inputs,(0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","            # Make predictions.\n","            prediction= model(inputs)\n","            # prediction= model(inputs,new_mask.to(device))\n","\n","            # Retrieve predictions indexes.\n","            _, predicted_class = torch.max(prediction.data, 1)\n","\n","            # Compute number of correct predictions.\n","            correct += (predicted_class == labels).float().sum().item()\n","            size+=len(prediction)\n","\n","    test_accuracy = correct / size\n","    print('Test accuracy: {}'.format(test_accuracy))\n","\n","\n","    model.eval()\n","    atks = [\n","        torchattacks.FGSM(model, eps=8/255),\n","        torchattacks.PGD(model, eps=8/255, alpha=2/225, steps=7, random_start=True),\n","    ]\n","    for i in [0,1]:\n","        correct = 0\n","        start = time.time()\n","        size = 0\n","        for images, labels in test_loader:   \n","            adv_images = atks[i](images, labels)\n","            adv_images = transforms.functional.normalize(adv_images,(0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","            labels = labels.to(device)\n","            outputs = model(adv_images)\n","            # outputs = model(adv_images,new_mask.to(device))\n","            _, pre = torch.max(outputs.data, 1)\n","            correct += (pre == labels).float().sum().item()\n","            size+=len(labels)\n","\n","        # print('Total elapsed time (sec): %.2f' % (time.time() - start))\n","        print('Robust accuracy: %.2f ' % (correct / size))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c9aPdrEK0w-L","executionInfo":{"status":"ok","timestamp":1647269865216,"user_tz":-60,"elapsed":512036,"user":{"displayName":"Khiem Nguyen Trong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11317374952968502408"}},"outputId":"e85d9762-7a45-40d1-b5c1-31f4b7dc1971"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy: 0.741\n","Robust accuracy: 0.33 \n","Robust accuracy: 0.30 \n","Test accuracy: 0.715\n","Robust accuracy: 0.35 \n","Robust accuracy: 0.32 \n","Test accuracy: 0.7358\n","Robust accuracy: 0.32 \n","Robust accuracy: 0.28 \n","Test accuracy: 0.738\n","Robust accuracy: 0.33 \n","Robust accuracy: 0.30 \n"]}]},{"cell_type":"code","source":["np.array([0.7432,0.696,0.7376,0.7368]).mean(), \\\n","np.array([0.38 ,0.37, 0.33,0.35]).mean(), \\\n","np.array([0.36,0.36,0.29,0.33]).mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-toBQf3B21FC","executionInfo":{"status":"ok","timestamp":1647269237727,"user_tz":-60,"elapsed":11,"user":{"displayName":"Khiem Nguyen Trong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11317374952968502408"}},"outputId":"a1683e82-8de7-4cb4-b359-6367386ac9e0"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.7284, 0.35750000000000004, 0.335)"]},"metadata":{},"execution_count":10}]}]}